{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import types\n",
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\tools\\\\hadoop-3.2.0'\n",
    "\n",
    "starting_file = \"index.parquet\"\n",
    "\n",
    "files = ['demographics.parquet', 'economy.parquet', 'geography.parquet', 'hospitalizations.parquet', 'mobility.parquet', \n",
    "         'google-search-trends.parquet', 'lawatlas-emergency-declarations.parquet', 'epidemiology.parquet', \n",
    "         'health.parquet', 'vaccinations.parquet', 'weather.parquet', 'by-age.parquet', 'by-sex.parquet']\n",
    "\n",
    "gcs_path = \"gs://cv19-453102-bucket/raw/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "if spark:\n",
    "    spark.stop()\n",
    "    print(\"Stopped session\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = r\"C:\\Users\\nbwan\\cv19pipeline\\airflow\\keys\\my-creds.json\"\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", r\"C:\\Users\\nbwan\\cv19pipeline\\lib\\gcs-connector3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_open_data = spark.read.parquet('gs://cv19-453102-bucket/raw/index.parquet')\n",
    "df_google_open_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def data_unification(df_google_open_data, files, gcs_path):\n",
    "    \"\"\"\n",
    "    Reads multiple CSV files and joins them on 'location_key' into df_google_open_data.\n",
    "\n",
    "    Args:\n",
    "    - df_google_open_data (DataFrame): Initial DataFrame to join with.\n",
    "    - files (list): List of CSV filenames.\n",
    "    - gcs_path (str): Path to GCS where files are stored.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Unified DataFrame with all files joined.\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = f\"{gcs_path}{file}\"\n",
    "        \n",
    "        # Read CSV with schema inference\n",
    "        df = spark.read.parquet(file_path)\n",
    "\n",
    "        # Ensure 'location_key' exists in both DataFrames\n",
    "        if \"location_key\" not in df.columns:\n",
    "            print(f\"Skipping {file}: 'location_key' column missing.\")\n",
    "            continue\n",
    "\n",
    "        # Optional: Broadcast smaller tables for performance\n",
    "        if df.count() < 500000:  # Adjust threshold as needed\n",
    "            df = broadcast(df)\n",
    "\n",
    "        # Perform inner join\n",
    "        df_google_open_data = df_google_open_data.join(df, on=\"location_key\", how=\"inner\")\n",
    "        print(f\"Inserted {file} into df_google_open_data\")\n",
    "\n",
    "    return df_google_open_data  # Ensure final DataFrame is returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unified = data_unification(df_google_open_data, files, gcs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unified.limit(10).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
